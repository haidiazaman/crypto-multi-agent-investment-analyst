import time
from typing import List, Literal
from src.models.base import BaseLLM
from src.tools.base import AgentTool
from langgraph.graph import StateGraph, START, END, MessagesState
from langchain_core.messages import SystemMessage, ToolMessage, BaseMessage, HumanMessage, AIMessage


class Agent:
    """
    Agent with pluggable LLM and tools
    
    Responsibilities:
    - Bind tools to the model (once at initialization)
    - Orchestrate the LLM â†’ Tool â†’ LLM loop
    - Use LLM's parsing logic for tool calls
    """
    
    def __init__(self, name: str, llm: BaseLLM, tools: List[AgentTool], system_prompt: str):
        self.name = name
        self.llm = llm
        self.tools = tools
        self.system_prompt = system_prompt
        
        # Convert tools to LangChain format
        self.langchain_tools = [tool.to_langchain_tool() for tool in tools]
        self.tools_by_name = {t.name: t for t in tools}
        
        # Bind tools to model once at initialization
        if self.langchain_tools:
            self._model_with_tools = llm._model.bind_tools(self.langchain_tools)
        else:
            self._model_with_tools = llm._model
        
        # Build the agent graph
        self.graph = self._build_graph()
    
    def _llm_call(self, state: dict):
        """LLM node - invokes model and parses tool calls"""
        messages = [SystemMessage(content=self.system_prompt)] + state["messages"]
        
        response = self._model_with_tools.invoke(messages)
        parsed_response = self.llm.parse_tool_calls(response)
        
        return {"messages": [parsed_response]}
    
    def _tool_node(self, state: dict):
        """Tool execution node"""
        result = []
        last_message = state["messages"][-1]
        
        for tool_call in last_message.tool_calls:
            tool = self.tools_by_name[tool_call["name"]]
            observation = tool.execute(**tool_call["args"])
            result.append(
                ToolMessage(
                    content=str(observation),
                    tool_call_id=tool_call["id"]
                )
            )
        return {"messages": result}
    
    def _should_continue(self, state: MessagesState) -> Literal["tool_node", END]: # type: ignore
        """Routing logic: continue to tools or end"""
        last_message = state["messages"][-1]
        if last_message.tool_calls:
            return "tool_node"
        return END
    
    def _build_graph(self):
        """Build the LangGraph agent"""
        builder = StateGraph(MessagesState)
        
        # Add nodes
        builder.add_node("llm_call", self._llm_call)
        builder.add_node("tool_node", self._tool_node)
        
        # Add edges
        builder.add_edge(START, "llm_call")
        builder.add_conditional_edges(
            "llm_call",
            self._should_continue,
            ["tool_node", END]
        )
        builder.add_edge("tool_node", "llm_call")
        
        return builder.compile()
    
    def invoke(self, messages: List[BaseMessage], stream_mode: str = "values"):
        """Run the agent"""
        return self.graph.invoke({"messages": messages}, stream_mode=stream_mode)
    
    def _stream_final_response(self, content: str):
        """Stream text content word by word"""
        words = content.split()
        print("\n")
        for word in words:
            print(word, end=" ", flush=True)
            time.sleep(0.005)  # Adjust speed (0.05s = 20 words/sec)
        print("\n")  # Newline at end

    def stream(self, messages: List[BaseMessage], stream_mode: str = "values"):
        """Run the agent in streaming mode"""
        
        # TODO: but can we see what the agent is thinking before the first Tool call?

        print("\n")
        tool_call_id_mapping = {} # id: name --> for functions
        previous_len = len(messages)  # Track how many messages we've seen
        for event in self.graph.stream({"messages": messages}, stream_mode=stream_mode):
            current_messages = event["messages"]
            new_messages = current_messages[previous_len:] # Get only NEW messages since last event
            previous_len = len(current_messages)
            
            for message in new_messages:
                if isinstance(message, AIMessage):
                    if message.tool_calls:
                        for tool_call in message.tool_calls:
                            tool_call_id_mapping[tool_call["id"]] = tool_call['name']
                            print(f"ğŸ”§ testing 123 Calling: {tool_call['name']}, Args: {str(tool_call['args'])[:100]}...")                    
                    elif message.content:
                        print(f"\nğŸ’¬ {message.content}")
                        # self._stream_final_response(message.content)
                
                # elif isinstance(message, ToolMessage):
                #     tool_id = message.tool_call_id
                #     function_name = tool_call_id_mapping[tool_id]
                #     print(f"output of {function_name}: {message.content}")

    async def astream(self, messages: List[BaseMessage]):
        """
        Async stream
        Run the agent in streaming mode with token-level streaming
        To run you need to use asyncio --> asyncio.run(agent.astream(messages))
        """
        async for event in self.graph.astream_events({"messages": messages}, version="v2"):
            kind = event["event"]
            
            # Tool calls
            if kind == "on_chat_model_stream":
                chunk = event["data"]["chunk"]
                
                if hasattr(chunk, 'tool_calls') and chunk.tool_calls:
                    # Tool call detected
                    for tc in chunk.tool_calls:
                        if tc['name']:
                            print(f"\nğŸ”§ Calling: {tc['name']}\n")
                
                elif chunk.content:
                    print(chunk.content, end="", flush=True) # Stream final response token by token

            # Capture final state (keep updating until last one)
            if kind == "on_chain_end":
                output = event["data"]["output"]
                if isinstance(output, dict) and "messages" in output:
                    final_messages = output["messages"]
        
        # Return last AI message
        if final_messages and len(final_messages) > 0:
            last_msg = final_messages[-1]
            return last_msg.content if hasattr(last_msg, 'content') else str(last_msg)
        
        return ""

    def conversation(self):
        """Run synchronous conversation loop with memory and streaming"""
        messages: list[BaseMessage] = []
        
        while True:
            user_input = input("\nâ“ ask me smth: ")
            if user_input.lower() in ["/bye", "exit", "quit"]:
                print("\nğŸ‘‹ End of conversation, bye!")
                break
            
            messages.append(HumanMessage(content=user_input))
            self.stream(messages=messages)
            print()  # Newline after response

    async def aconversation(self):
        """Run async conversation loop with memory and streaming"""
        messages: list[BaseMessage] = []
        
        while True:
            user_input = input("\nâ“ ask me smth: ")
            if user_input.lower() in ["/bye", "exit", "quit"]:
                print("\nğŸ‘‹ End of conversation, bye!")
                break
            
            messages.append(HumanMessage(content=user_input))
            response = await self.astream(messages=messages)
            messages.append(AIMessage(content=response))      # â† Add to history
            print()  # Newline after response